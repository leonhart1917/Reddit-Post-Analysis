{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Required Package"
      ],
      "metadata": {
        "id": "MwMzrHjRN29Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install praw python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmPYFx7qMxXj",
        "outputId": "bb161af7-e323-4ab7-c74a-f8f54fa719d0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.12/dist-packages (from praw) (1.9.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from prawcore<3,>=2.4->praw) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.10.5)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Required Package"
      ],
      "metadata": {
        "id": "JZJABB8PN7_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import csv\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_1F4u0yM3u1",
        "outputId": "5196b887-68e2-4b83-b810-e933d6c88ef7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting google Drive"
      ],
      "metadata": {
        "id": "Jj2WAme0N_ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLS8UaiONJD1",
        "outputId": "6b9e29d0-934b-4d9a-c191-a971c2142014"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Environment Variables\n",
        "Load Reddit API credentials from the environment file.\n",
        "### And\n",
        "### Authenticate with Reddit API\n",
        "Establish connection to Reddit using PRAW with the loaded credentials."
      ],
      "metadata": {
        "id": "sK_08Xh2OFN2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq7V0vDfMnkQ",
        "outputId": "b7eadf40-0ee8-42d8-c0ff-3d729bfba367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Reddit API authenticated successfully!\n",
            "Read-only mode: True\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv(\"reddit_api.env\") #change the name to your actual reddit env file name\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=os.getenv(\"REDDIT_CLIENT_ID\"),\n",
        "    client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
        "    user_agent=os.getenv(\"REDDIT_USER_AGENT\")\n",
        ")\n",
        "print(\"‚úÖ Reddit API authenticated successfully!\")\n",
        "print(f\"Read-only mode: {reddit.read_only}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Define Data Collection Function\n",
        "\n",
        "Create a function to download recent posts from a specified subreddit with proper error handling and improvements.\n"
      ],
      "metadata": {
        "id": "UrsyuqkLOMS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_recent_posts(subreddits, limit=100, filename=\"reddit_data.csv\"):\n",
        "    \"\"\"\n",
        "    Download 'hot' posts from one or more subreddits and write to a single CSV.\n",
        "\n",
        "    Args:\n",
        "        subreddits (str | list[str]): subreddit name or list of names\n",
        "        limit (int): posts per subreddit\n",
        "        filename (str): output CSV file\n",
        "    \"\"\"\n",
        "    import csv\n",
        "\n",
        "    # normalize input\n",
        "    if isinstance(subreddits, str):\n",
        "        subreddits = [subreddits]\n",
        "    if not isinstance(subreddits, list) or not subreddits:\n",
        "        raise ValueError(\"Provide at least one subreddit name.\")\n",
        "\n",
        "    try:\n",
        "        with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            # header once\n",
        "            writer.writerow([\n",
        "                \"Title\",\"Score\",\"Upvote_ratio\",\"Num_comments\",\"Author\",\"Subreddit\",\n",
        "                \"URL\",\"Permalink\",\"Created_utc\",\"Is_self\",\"Selftext\",\"Flair\",\"Domain\",\"Search_query\"\n",
        "            ])\n",
        "\n",
        "            total = 0\n",
        "            for sub in subreddits:\n",
        "                print(f\"üì• Collecting hot posts from r/{sub} ...\")\n",
        "                for post in reddit.subreddit(sub).hot(limit=limit):\n",
        "                    author = str(post.author) if post.author else \"[deleted]\"\n",
        "                    selftext = post.selftext[:500] if getattr(post, \"selftext\", None) else None\n",
        "                    flair = getattr(post, \"link_flair_text\", None)\n",
        "                    domain = getattr(post, \"domain\", None)\n",
        "\n",
        "                    writer.writerow([\n",
        "                        post.title,\n",
        "                        post.score,\n",
        "                        getattr(post, \"upvote_ratio\", None),\n",
        "                        post.num_comments,\n",
        "                        author,\n",
        "                        str(post.subreddit),\n",
        "                        post.url,\n",
        "                        f\"https://reddit.com{post.permalink}\",\n",
        "                        int(post.created_utc),\n",
        "                        post.is_self,\n",
        "                        selftext,\n",
        "                        flair,\n",
        "                        domain,\n",
        "                        None  # hot posts ‚Üí no search query\n",
        "                    ])\n",
        "                    total += 1\n",
        "\n",
        "        print(f\"‚úÖ Saved {total} posts from {len(subreddits)} subreddits to '{filename}'.\")\n",
        "        return True\n",
        "\n",
        "    except praw.exceptions.PRAWException as e:\n",
        "        print(f\"‚ùå Reddit API error: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Unexpected error: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "print(\"‚úÖ Function defined successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIfi_pa8Q9Ac",
        "outputId": "df433fd1-4fdc-4bee-8f23-920481fabfd3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Function defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Search Query Function"
      ],
      "metadata": {
        "id": "mZnCH1QtOfOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_posts(query, subreddits, limit=50, filename=\"reddit_search.csv\"):\n",
        "    \"\"\"\n",
        "    Search Reddit posts across one or more subreddits and save results to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        query (str): Keyword to search for (e.g., \"GPT-4\")\n",
        "        subreddits (list): List of subreddit names (e.g., [\"datascience\", \"MachineLearning\"])\n",
        "        limit (int): Number of posts per subreddit to fetch\n",
        "        filename (str): Output CSV file name\n",
        "    \"\"\"\n",
        "\n",
        "    import csv\n",
        "    import os\n",
        "\n",
        "    if not query or not isinstance(query, str):\n",
        "        raise ValueError(\"Query must be a non-empty string.\")\n",
        "    if isinstance(subreddits, str):\n",
        "        subreddits = [subreddits]\n",
        "    if not isinstance(subreddits, list) or len(subreddits) == 0:\n",
        "        raise ValueError(\"Please provide at least one subreddit name.\")\n",
        "\n",
        "    try:\n",
        "        with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\n",
        "                \"Title\", \"Score\", \"Upvote_ratio\", \"Num_comments\", \"Author\", \"Subreddit\",\n",
        "                \"URL\", \"Permalink\", \"Created_utc\", \"Is_self\", \"Selftext\",\n",
        "                \"Flair\", \"Domain\", \"Search_query\"\n",
        "            ])\n",
        "\n",
        "            total_count = 0\n",
        "            for sub in subreddits:\n",
        "                subreddit = reddit.subreddit(sub)\n",
        "                print(f\"üîç Searching '{query}' in r/{sub} ...\")\n",
        "\n",
        "                for post in subreddit.search(query, limit=limit):\n",
        "                    author_name = str(post.author) if post.author else \"[deleted]\"\n",
        "                    selftext = post.selftext[:500] if post.selftext else None\n",
        "                    flair = getattr(post, \"link_flair_text\", None)\n",
        "                    domain = getattr(post, \"domain\", None)\n",
        "\n",
        "                    writer.writerow([\n",
        "                        post.title,\n",
        "                        post.score,\n",
        "                        getattr(post, \"upvote_ratio\", None),\n",
        "                        post.num_comments,\n",
        "                        author_name,\n",
        "                        str(post.subreddit),\n",
        "                        post.url,\n",
        "                        f\"https://reddit.com{post.permalink}\",\n",
        "                        int(post.created_utc),\n",
        "                        post.is_self,\n",
        "                        selftext,\n",
        "                        flair,\n",
        "                        domain,\n",
        "                        query  # ‚úÖ this column shows which keyword was searched\n",
        "                    ])\n",
        "                    total_count += 1\n",
        "\n",
        "            print(f\"‚úÖ Completed search for '{query}'. Total posts saved: {total_count}\")\n",
        "            return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during search: {e}\")\n",
        "        return False\n",
        "print(\"‚úÖ Function defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHq_b3eHKX7V",
        "outputId": "866d75e1-9c76-4a98-f33b-a7a99bb9c3fb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Function defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure Parameters\n",
        "\n",
        "Set the parameters for data collection - subreddit, number of posts, and output filename."
      ],
      "metadata": {
        "id": "Boaq0hDmOo3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration parameters\n",
        "subreddit = [\"datascience\", \"MachineLearning\", \"learnpython\"] # Subreddit to download posts from\n",
        "limit = 100  # Number of posts to download\n",
        "filename = \"reddit_data.csv\"  # Name of the CSV file\n",
        "\n",
        "print(f\"üìã Configuration:\")\n",
        "print(f\"   Subreddit: r/{subreddit}\")\n",
        "print(f\"   Posts to download: {limit}\")\n",
        "print(f\"   Output file: {filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvoHoIk6PgK0",
        "outputId": "0a9424d4-fe22-4e20-b909-52b28c18f3a5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìã Configuration:\n",
            "   Subreddit: r/['datascience', 'MachineLearning', 'learnpython']\n",
            "   Posts to download: 100\n",
            "   Output file: reddit_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute Data Collection\n",
        "\n",
        "Run the data collection function to download cricket posts from Reddit."
      ],
      "metadata": {
        "id": "y5shtBdGO2ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the data collection\n",
        "success = download_recent_posts(subreddit, limit, filename)\n",
        "\n",
        "if success:\n",
        "    print(\"\\nüéâ Data collection completed successfully!\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Data collection failed. Please check the error messages above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLC_OWvWRGF9",
        "outputId": "98c77d2b-e38c-492d-9c7c-69d4332915a4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Collecting hot posts from r/datascience ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Collecting hot posts from r/MachineLearning ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Collecting hot posts from r/learnpython ...\n",
            "‚úÖ Saved 300 posts from 3 subreddits to 'reddit_data.csv'.\n",
            "\n",
            "üéâ Data collection completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search Query Function output"
      ],
      "metadata": {
        "id": "Qp6p9eQDOsn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"GPT-4\"\n",
        "subreddits = [\"datascience\", \"MachineLearning\", \"learnpython\"]\n",
        "filename_search = \"reddit_search.csv\"\n",
        "\n",
        "# Run the search function\n",
        "success = search_posts(query=query, subreddits=subreddits, limit=30, filename=filename_search)\n",
        "\n",
        "if success:\n",
        "    print(f\"üéâ Search results saved to {filename_search}\")\n",
        "else:\n",
        "    print(\"‚ùå Something went wrong during search.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8DNsv5NKzwc",
        "outputId": "e3fb85c9-e459-47ab-d020-0ffa3a906158"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Searching 'GPT-4' in r/datascience ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Searching 'GPT-4' in r/MachineLearning ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Searching 'GPT-4' in r/learnpython ...\n",
            "‚úÖ Completed search for 'GPT-4'. Total posts saved: 90\n",
            "üéâ Search results saved to reddit_search.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge two datasets"
      ],
      "metadata": {
        "id": "W2TFfpTyO5HP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load both CSVs\n",
        "df_hot = pd.read_csv(\"reddit_data.csv\")\n",
        "df_search = pd.read_csv(\"reddit_search.csv\")\n",
        "\n",
        "# Combine and deduplicate\n",
        "df_final = pd.concat([df_hot, df_search], ignore_index=True)\n",
        "df_final.drop_duplicates(subset=[\"Permalink\"], inplace=True)\n",
        "\n",
        "# Save the final combined file\n",
        "df_final.to_csv(\"reddit_data.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Final reddit_data.csv saved with {len(df_final)} unique posts.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZesnLOWK92N",
        "outputId": "d8f163d2-cf9c-4a21-854b-592d9fac0536"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Final reddit_data.csv saved with 387 unique posts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Analysis"
      ],
      "metadata": {
        "id": "UDElp8GwO9PH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "filename = \"reddit_data.csv\"\n",
        "# Check if file exists and load data\n",
        "if os.path.exists(filename):\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(filename)\n",
        "\n",
        "    print(f\"üìä Dataset Overview:\")\n",
        "    print(f\"   Total posts: {len(df)}\")\n",
        "    print(f\"   Columns: {list(df.columns)}\")\n",
        "    print(f\"   File size: {os.path.getsize(filename)} bytes\")\n",
        "\n",
        "    print(f\"\\nüìù Sample Posts:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Display first 3 posts\n",
        "    for i, row in df.head(3).iterrows():\n",
        "        print(f\"\\nPost {i+1}:\")\n",
        "        print(f\"Title: {row['Title'][:80]}...\")\n",
        "        print(f\"Author: {row['Author']}\")\n",
        "        print(f\"Score: {row['Score']}\")\n",
        "        print(f\"URL: {row['URL']}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    # Basic statistics\n",
        "    print(f\"\\nüìà Basic Statistics:\")\n",
        "    print(f\"   Average score: {df['Score'].mean():.2f}\")\n",
        "    print(f\"   Highest score: {df['Score'].max()}\")\n",
        "    print(f\"   Lowest score: {df['Score'].min()}\")\n",
        "\n",
        "else:\n",
        "    print(f\"‚ùå File '{filename}' not found!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mskjEEtORacH",
        "outputId": "e295d09f-670e-4c43-f476-ef00dc93bd81"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Dataset Overview:\n",
            "   Total posts: 387\n",
            "   Columns: ['Title', 'Score', 'Upvote_ratio', 'Num_comments', 'Author', 'Subreddit', 'URL', 'Permalink', 'Created_utc', 'Is_self', 'Selftext', 'Flair', 'Domain', 'Search_query']\n",
            "   File size: 286177 bytes\n",
            "\n",
            "üìù Sample Posts:\n",
            "==================================================\n",
            "\n",
            "Post 1:\n",
            "Title: Weekly Entering & Transitioning - Thread 03 Nov, 2025 - 10 Nov, 2025...\n",
            "Author: AutoModerator\n",
            "Score: 1\n",
            "URL: https://www.reddit.com/r/datascience/comments/1on34xg/weekly_entering_transitioning_thread_03_nov_2025/\n",
            "------------------------------\n",
            "\n",
            "Post 2:\n",
            "Title: How would you turn a working Jupyter pipeline into a small web app?...\n",
            "Author: Proof_Wrap_2150\n",
            "Score: 26\n",
            "URL: https://www.reddit.com/r/datascience/comments/1ommxv4/how_would_you_turn_a_working_jupyter_pipeline/\n",
            "------------------------------\n",
            "\n",
            "Post 3:\n",
            "Title: Is it too early to accept an internship offer?...\n",
            "Author: LilParkButt\n",
            "Score: 18\n",
            "URL: https://www.reddit.com/r/datascience/comments/1om9zgm/is_it_too_early_to_accept_an_internship_offer/\n",
            "------------------------------\n",
            "\n",
            "üìà Basic Statistics:\n",
            "   Average score: 79.45\n",
            "   Highest score: 2473\n",
            "   Lowest score: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Analysis"
      ],
      "metadata": {
        "id": "_N-a7_rXPBDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional analysis (optional)\n",
        "if 'df' in locals() and not df.empty:\n",
        "    print(\"üîç Additional Analysis:\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Top scoring posts\n",
        "    top_posts = df.nlargest(5, 'Score')[['Title', 'Score', 'Author']]\n",
        "    print(\"\\nüèÜ Top 5 Posts by Score:\")\n",
        "    for i, (_, row) in enumerate(top_posts.iterrows(), 1):\n",
        "        print(f\"{i}. {row['Title'][:60]}... (Score: {row['Score']}, Author: {row['Author']})\")\n",
        "\n",
        "    # Most active authors\n",
        "    author_counts = df['Author'].value_counts().head(5)\n",
        "    print(\"\\nüë• Most Active Authors:\")\n",
        "    for author, count in author_counts.items():\n",
        "        print(f\"   {author}: {count} posts\")\n",
        "\n",
        "    # Posts with text content\n",
        "    posts_with_text = df[df['Selftext'].str.len() > 0]\n",
        "    print(f\"\\nüìù Posts with text content: {len(posts_with_text)} out of {len(df)}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No data available for analysis\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40UtNCp0RiU9",
        "outputId": "86c9cfe2-c11d-4852-d52c-6bc69f839330"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Additional Analysis:\n",
            "========================================\n",
            "\n",
            "üèÜ Top 5 Posts by Score:\n",
            "1. Why do new analysts often ignore R?... (Score: 2473, Author: ElectrikMetriks)\n",
            "2. My Data Science Manifesto from a Self Taught Data Scientist... (Score: 2078, Author: irndk10)\n",
            "3. [D] Anyone else witnessing a panic inside NLP orgs of big te... (Score: 1381, Author: thrwsitaway4321)\n",
            "4. I investigated the Underground Economy of Glassdoor Reviews... (Score: 1159, Author: ibsurvivors)\n",
            "5. [N] OpenAI may have benchmarked GPT-4‚Äôs coding ability on it... (Score: 1002, Author: Balance-)\n",
            "\n",
            "üë• Most Active Authors:\n",
            "   AutoModerator: 11 posts\n",
            "   DeepAnalyze: 4 posts\n",
            "   WarChampion90: 4 posts\n",
            "   seraine: 3 posts\n",
            "   Singularian2501: 3 posts\n",
            "\n",
            "üìù Posts with text content: 357 out of 387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J_u0OOPMSSBE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
